import { NextRequest, NextResponse } from 'next/server';
import { llmService } from '@/lib/services/LLMService';

export async function POST(request: NextRequest) {
  try {
    const { query, context } = await request.json();

    if (!query || typeof query !== 'string' || query.trim().length === 0) {
      return NextResponse.json(
        { error: 'ì§ˆë¬¸ì´ í•„ìš”í•©ë‹ˆë‹¤.' },
        { status: 400 }
      );
    }

    console.log(`ğŸ§ª LLM í…ŒìŠ¤íŠ¸ ìš”ì²­: "${query}"`);

    // LLM ì„œë¹„ìŠ¤ ìƒíƒœ í™•ì¸
    const isAvailable = await llmService.checkOllamaStatus();
    
    if (!isAvailable) {
      return NextResponse.json({
        success: false,
        error: 'Ollama ì„œë¹„ìŠ¤ê°€ ì‚¬ìš© ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤.',
        suggestion: 'Ollama ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.'
      });
    }

    // LLM í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    const testContext = context || 'Meta ê´‘ê³  ì •ì±…ì— ëŒ€í•œ ì¼ë°˜ì ì¸ ì •ë³´ì…ë‹ˆë‹¤.';
    const response = await llmService.generateProfessionalAnswer(query, testContext);
    
    // ë‹µë³€ í’ˆì§ˆ ê²€ì¦
    const validation = llmService.validateAnswer(response.answer, query);

    console.log(`âœ… LLM í…ŒìŠ¤íŠ¸ ì™„ë£Œ: ${response.processingTime}ms, ì‹ ë¢°ë„: ${response.confidence}`);

    return NextResponse.json({
      success: true,
      response: {
        answer: response.answer,
        confidence: response.confidence,
        processingTime: response.processingTime,
        model: response.model,
        validation: {
          isValid: validation.isValid,
          issues: validation.issues,
          suggestions: validation.suggestions
        }
      }
    });

  } catch (error) {
    console.error('LLM í…ŒìŠ¤íŠ¸ ì˜¤ë¥˜:', error);
    
    return NextResponse.json(
      { 
        success: false,
        error: 'LLM í…ŒìŠ¤íŠ¸ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.',
        details: error instanceof Error ? error.message : String(error)
      },
      { status: 500 }
    );
  }
}
