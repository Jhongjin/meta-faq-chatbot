import { NextRequest, NextResponse } from 'next/server';

// ê¸°ë³¸ í—¤ë” ì„¤ì •
const headers = {
  'Content-Type': 'application/json',
  'Access-Control-Allow-Origin': '*',
  'Access-Control-Allow-Methods': 'GET, POST, OPTIONS',
  'Access-Control-Allow-Headers': 'Content-Type',
};

// OPTIONS ë©”ì„œë“œ
export async function OPTIONS() {
  return new NextResponse(null, {
    status: 200,
    headers,
  });
}

// GET ë©”ì„œë“œ - ë¡œì»¬ í…ŒìŠ¤íŠ¸ìš© ëª¨ì˜ ì‘ë‹µ
export async function GET() {
  try {
    console.log('ğŸ” ë¡œì»¬ í…ŒìŠ¤íŠ¸ ëª¨ë“œ - ëª¨ì˜ Ollama ì„œë²„ ì‘ë‹µ');
    
    // ëª¨ì˜ ëª¨ë¸ ëª©ë¡
    const mockModels = [
      {
        name: 'tinyllama:1.1b',
        id: 'mock-tinyllama-1.1b',
        size: 637 * 1024 * 1024, // 637MB
        modified_at: new Date().toISOString()
      },
      {
        name: 'llama2:7b',
        id: 'mock-llama2-7b',
        size: 3800 * 1024 * 1024, // 3.8GB
        modified_at: new Date().toISOString()
      },
      {
        name: 'mistral:7b',
        id: 'mock-mistral-7b',
        size: 4400 * 1024 * 1024, // 4.4GB
        modified_at: new Date().toISOString()
      }
    ];

    const response = {
      success: true,
      message: 'ë¡œì»¬ í…ŒìŠ¤íŠ¸ ëª¨ë“œ - ëª¨ì˜ Ollama ì„œë²„ ì‘ë‹µ',
      timestamp: new Date().toISOString(),
      server: {
        healthy: true,
        baseUrl: 'http://localhost:11434',
        actualUrl: 'http://localhost:11434',
        availableModels: mockModels.map(model => ({
          name: model.name,
          size: `${(model.size / 1024 / 1024 / 1024).toFixed(2)}GB`,
          modifiedAt: model.modified_at
        }))
      },
      methods: ['GET', 'POST', 'OPTIONS'],
      version: 'ollama-mock-v1',
      endpoint: '/api/ollama/local-test',
      mode: 'mock'
    };

    console.log('ğŸ“¤ ëª¨ì˜ API ì‘ë‹µ:', {
      success: response.success,
      serverHealthy: response.server.healthy,
      modelsCount: response.server.availableModels.length
    });

    return NextResponse.json(response, {
      status: 200,
      headers,
    });

  } catch (error) {
    console.error('âŒ ë¡œì»¬ í…ŒìŠ¤íŠ¸ API ì˜¤ë¥˜:', error);
    
    return NextResponse.json({
      success: false,
      error: 'ë¡œì»¬ í…ŒìŠ¤íŠ¸ API ì˜¤ë¥˜',
      details: error instanceof Error ? error.message : String(error)
    }, {
      status: 500,
      headers,
    });
  }
}

// POST ë©”ì„œë“œ - ë¡œì»¬ í…ŒìŠ¤íŠ¸ìš© ëª¨ì˜ ì±„íŒ… ì‘ë‹µ
export async function POST(request: NextRequest) {
  try {
    const body = await request.json();
    const { message, model = 'tinyllama:1.1b' } = body;

    console.log('ğŸ¤– ë¡œì»¬ í…ŒìŠ¤íŠ¸ ëª¨ì˜ ì±„íŒ…:', { message, model });

    // ëª¨ì˜ ì‘ë‹µ ìƒì„±
    const mockResponse = `[ëª¨ì˜ ì‘ë‹µ] ì•ˆë…•í•˜ì„¸ìš”! ì €ëŠ” ${model} ëª¨ë¸ì…ë‹ˆë‹¤. 
    
ê·€í•˜ì˜ ì§ˆë¬¸: "${message}"

ì´ê²ƒì€ ë¡œì»¬ í…ŒìŠ¤íŠ¸ ëª¨ë“œì—ì„œ ìƒì„±ëœ ëª¨ì˜ ì‘ë‹µì…ë‹ˆë‹¤. 
ì‹¤ì œ Ollama ì„œë²„ê°€ ì—°ê²°ë˜ë©´ ë” ì •í™•í•œ ë‹µë³€ì„ ì œê³µí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

í˜„ì¬ ì‚¬ìš© ì¤‘ì¸ ëª¨ë¸: ${model}
ì‘ë‹µ ìƒì„± ì‹œê°„: ${new Date().toLocaleString()}`;

    const response = {
      success: true,
      response: {
        message: mockResponse,
        model: model,
        processingTime: Math.floor(Math.random() * 1000) + 500, // 500-1500ms
        server: 'Mock Ollama (Local Test)',
        timestamp: new Date().toISOString()
      }
    };

    console.log('ğŸ“¤ ëª¨ì˜ ì±„íŒ… ì‘ë‹µ ì™„ë£Œ');

    return NextResponse.json(response, {
      status: 200,
      headers,
    });

  } catch (error) {
    console.error('âŒ ë¡œì»¬ í…ŒìŠ¤íŠ¸ ì±„íŒ… ì˜¤ë¥˜:', error);
    
    return NextResponse.json({
      success: false,
      error: 'ë¡œì»¬ í…ŒìŠ¤íŠ¸ ì±„íŒ… ì˜¤ë¥˜',
      details: error instanceof Error ? error.message : String(error)
    }, {
      status: 500,
      headers,
    });
  }
}

