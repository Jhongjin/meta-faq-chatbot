"""
Railway + Ollama ê¸°ë°˜ Meta FAQ AI ì±—ë´‡ ë°±ì—”ë“œ
FastAPIë¥¼ ì‚¬ìš©í•œ RAG ì‹œìŠ¤í…œ êµ¬í˜„
"""

from fastapi import FastAPI, HTTPException, UploadFile, File, Form, Depends
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse
from pydantic import BaseModel
from typing import List, Optional, Dict, Any
import os
import json
import asyncio
import aiohttp
from datetime import datetime
import logging

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = FastAPI(
    title="Meta FAQ AI Chatbot API",
    description="Railway + Ollama ê¸°ë°˜ RAG ì‹œìŠ¤í…œ",
    version="1.0.0"
)

# CORS ì„¤ì •
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # í”„ë¡œë•ì…˜ì—ì„œëŠ” íŠ¹ì • ë„ë©”ì¸ìœ¼ë¡œ ì œí•œ
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# í™˜ê²½ ë³€ìˆ˜
OLLAMA_BASE_URL = os.getenv("OLLAMA_BASE_URL", "http://localhost:11434")
EMBEDDING_MODEL = os.getenv("EMBEDDING_MODEL", "nomic-embed-text")
LLM_MODEL = os.getenv("LLM_MODEL", "llama3.2:3b")
SUPABASE_URL = os.getenv("SUPABASE_URL")
SUPABASE_KEY = os.getenv("SUPABASE_KEY")
EMBEDDING_DIM = int(os.getenv("EMBEDDING_DIM", "768"))
TOP_K = int(os.getenv("TOP_K", "5"))

# Pydantic ëª¨ë¸ë“¤
class ChatMessage(BaseModel):
    message: str
    conversation_id: Optional[str] = None

class ChatResponse(BaseModel):
    response: str
    sources: List[Dict[str, Any]]
    confidence: float
    processing_time: float
    model: str

class DocumentUpload(BaseModel):
    title: str
    content: str
    document_type: str
    metadata: Optional[Dict[str, Any]] = None

# Ollama í´ë¼ì´ì–¸íŠ¸
class OllamaClient:
    def __init__(self, base_url: str):
        self.base_url = base_url
    
    async def generate_embedding(self, text: str) -> List[float]:
        """í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„±"""
        try:
            async with aiohttp.ClientSession() as session:
                async with session.post(
                    f"{self.base_url}/api/embeddings",
                    json={"model": EMBEDDING_MODEL, "prompt": text}
                ) as response:
                    if response.status == 200:
                        data = await response.json()
                        return data.get("embedding", [])
                    else:
                        logger.error(f"Embedding API error: {response.status}")
                        return []
        except Exception as e:
            logger.error(f"Embedding generation error: {e}")
            return []
    
    async def generate_response(self, prompt: str, context: str = "") -> str:
        """LLM ì‘ë‹µ ìƒì„±"""
        try:
            full_prompt = f"""ë‹¤ìŒ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì •í™•í•˜ê³  ë„ì›€ì´ ë˜ëŠ” ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”.

ì»¨í…ìŠ¤íŠ¸:
{context}

ì‚¬ìš©ì ì§ˆë¬¸: {prompt}

ë‹µë³€:"""
            
            async with aiohttp.ClientSession() as session:
                async with session.post(
                    f"{self.base_url}/api/generate",
                    json={
                        "model": LLM_MODEL,
                        "prompt": full_prompt,
                        "stream": False
                    }
                ) as response:
                    if response.status == 200:
                        data = await response.json()
                        return data.get("response", "")
                    else:
                        logger.error(f"LLM API error: {response.status}")
                        return "ì£„ì†¡í•©ë‹ˆë‹¤. í˜„ì¬ ì„œë¹„ìŠ¤ì— ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤."
        except Exception as e:
            logger.error(f"LLM generation error: {e}")
            return "ì£„ì†¡í•©ë‹ˆë‹¤. í˜„ì¬ ì„œë¹„ìŠ¤ì— ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤."

# Supabase í´ë¼ì´ì–¸íŠ¸ (ê°„ë‹¨í•œ êµ¬í˜„)
class SupabaseClient:
    def __init__(self, url: str, key: str):
        self.url = url
        self.key = key
        self.headers = {
            "apikey": key,
            "Authorization": f"Bearer {key}",
            "Content-Type": "application/json"
        }
    
    async def search_similar_chunks(self, query_embedding: List[float], limit: int = 5) -> List[Dict[str, Any]]:
        """ìœ ì‚¬í•œ ë¬¸ì„œ ì²­í¬ ê²€ìƒ‰"""
        try:
            async with aiohttp.ClientSession() as session:
                async with session.post(
                    f"{self.url}/rest/v1/rpc/search_similar_chunks",
                    headers=self.headers,
                    json={
                        "query_embedding": query_embedding,
                        "match_threshold": 0.7,
                        "match_count": limit
                    }
                ) as response:
                    if response.status == 200:
                        data = await response.json()
                        return data
                    else:
                        logger.error(f"Supabase search error: {response.status}")
                        return []
        except Exception as e:
            logger.error(f"Supabase search error: {e}")
            return []

# í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” (ì§€ì—° ë¡œë”©)
def get_ollama_client():
    return OllamaClient(OLLAMA_BASE_URL)

def get_supabase_client():
    if SUPABASE_URL and SUPABASE_KEY:
        return SupabaseClient(SUPABASE_URL, SUPABASE_KEY)
    return None

# ì „ì—­ í´ë¼ì´ì–¸íŠ¸ (ì‹¤ì œ ì‚¬ìš© ì‹œì ì— ì´ˆê¸°í™”)
ollama_client = None
supabase_client = None

@app.get("/")
async def root():
    return {
        "status": "ok",
        "message": "AdMate Railway API is running"
    }

@app.get("/health")
async def health_check():
    """í—¬ìŠ¤ ì²´í¬ ì—”ë“œí¬ì¸íŠ¸"""
    try:
        # Ollama ì—°ê²° ìƒíƒœ í™•ì¸
        async with aiohttp.ClientSession() as session:
            async with session.get(f"{OLLAMA_BASE_URL}/api/tags", timeout=5.0) as response:
                ollama_status = "connected" if response.status == 200 else "disconnected"
    except Exception as e:
        logger.error(f"Ollama connection error: {e}")
        ollama_status = "disconnected"
    
    return {
        "status": "healthy",
        "ollama_status": ollama_status,
        "ollama_url": OLLAMA_BASE_URL,
        "supabase_url": SUPABASE_URL
    }

@app.post("/api/chat", response_model=ChatResponse)
async def chat_endpoint(chat_message: ChatMessage):
    """ì±„íŒ… ë©”ì‹œì§€ ì²˜ë¦¬"""
    global ollama_client, supabase_client
    
    # í´ë¼ì´ì–¸íŠ¸ ì§€ì—° ì´ˆê¸°í™” (ì•ˆì „í•œ ë°©ì‹)
    try:
        if not ollama_client:
            ollama_client = get_ollama_client()
    except Exception as e:
        logger.warning(f"Ollama client initialization failed: {e}")
        ollama_client = None
    
    try:
        if not supabase_client:
            supabase_client = get_supabase_client()
    except Exception as e:
        logger.warning(f"Supabase client initialization failed: {e}")
        supabase_client = None
    
    start_time = datetime.now()
    
    try:
        # 1. ì§ˆë¬¸ ì„ë² ë”© ìƒì„± (ë°±ì—… ì‹œìŠ¤í…œ í¬í•¨)
        try:
            if ollama_client:
                query_embedding = await ollama_client.generate_embedding(chat_message.message)
                if not query_embedding:
                    logger.warning("Embedding generation failed, using fallback")
                    return await fallback_chat_response(chat_message.message)
            else:
                logger.warning("Ollama client unavailable, using fallback")
                return await fallback_chat_response(chat_message.message)
        except Exception as e:
            logger.error(f"Embedding error: {e}, using fallback")
            return await fallback_chat_response(chat_message.message)
        
        # 2. ìœ ì‚¬í•œ ë¬¸ì„œ ê²€ìƒ‰
        search_results = []
        if supabase_client:
            search_results = await supabase_client.search_similar_chunks(query_embedding, TOP_K)
        
        # 3. ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
        context = ""
        sources = []
        
        for result in search_results:
            context += f"\n{result.get('content', '')}"
            sources.append({
                "title": result.get('title', 'Unknown'),
                "content": result.get('content', '')[:200] + "...",
                "similarity": result.get('similarity', 0.0),
                "source_type": result.get('source_type', 'document'),
                "document_type": result.get('document_type', 'file')
            })
        
        # 4. LLM ì‘ë‹µ ìƒì„±
        response_text = await ollama_client.generate_response(chat_message.message, context)
        
        # 5. ì‘ë‹µ ì‹œê°„ ê³„ì‚°
        processing_time = (datetime.now() - start_time).total_seconds()
        
        return ChatResponse(
            response=response_text,
            sources=sources,
            confidence=0.8,  # ê¸°ë³¸ê°’
            processing_time=processing_time,
            model=LLM_MODEL
        )
        
    except Exception as e:
        logger.error(f"Chat processing error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/upload")
async def upload_document(
    file: UploadFile = File(...),
    title: str = Form(...),
    document_type: str = Form(...)
):
    """ë¬¸ì„œ ì—…ë¡œë“œ ë° ì²˜ë¦¬"""
    try:
        # íŒŒì¼ ë‚´ìš© ì½ê¸°
        content = await file.read()
        content_text = content.decode('utf-8')
        
        # ë¬¸ì„œ ì²­í¬ ìƒì„± (ê°„ë‹¨í•œ êµ¬í˜„)
        chunks = [content_text[i:i+1000] for i in range(0, len(content_text), 1000)]
        
        # ê° ì²­í¬ì— ëŒ€í•´ ì„ë² ë”© ìƒì„± ë° ì €ì¥
        processed_chunks = []
        for i, chunk in enumerate(chunks):
            embedding = await ollama_client.generate_embedding(chunk)
            if embedding:
                processed_chunks.append({
                    "chunk_id": f"{title}_chunk_{i}",
                    "content": chunk,
                    "embedding": embedding,
                    "metadata": {
                        "title": title,
                        "document_type": document_type,
                        "chunk_index": i
                    }
                })
        
        return {
            "message": "ë¬¸ì„œê°€ ì„±ê³µì ìœ¼ë¡œ ì²˜ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤.",
            "chunks_processed": len(processed_chunks),
            "title": title
        }
        
    except Exception as e:
        logger.error(f"Document upload error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/models")
async def get_available_models():
    """ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ ì¡°íšŒ"""
    try:
        async with aiohttp.ClientSession() as session:
            async with session.get(f"{OLLAMA_BASE_URL}/api/tags") as response:
                if response.status == 200:
                    data = await response.json()
                    return {"models": data.get("models", [])}
                else:
                    return {"models": []}
    except Exception as e:
        logger.error(f"Model list error: {e}")
        return {"models": []}

@app.post("/api/pull-model")
async def pull_model(model_name: str):
    """Ollama ëª¨ë¸ ë‹¤ìš´ë¡œë“œ"""
    try:
        async with aiohttp.ClientSession() as session:
            async with session.post(
                f"{OLLAMA_BASE_URL}/api/pull",
                json={"name": model_name},
                timeout=aiohttp.ClientTimeout(total=600)  # 10ë¶„ íƒ€ì„ì•„ì›ƒ
            ) as response:
                if response.status == 200:
                    return {"status": "success", "message": f"Model {model_name} pulled successfully"}
                else:
                    error_text = await response.text()
                    return {"status": "error", "message": f"Failed to pull model: {error_text}"}
    except Exception as e:
        logger.error(f"Model pull error: {e}")
        return {"status": "error", "message": str(e)}

@app.post("/api/setup-models")
async def setup_required_models():
    """í•„ìˆ˜ ëª¨ë¸ë“¤ ìë™ ë‹¤ìš´ë¡œë“œ"""
    models_to_pull = [EMBEDDING_MODEL, LLM_MODEL]
    results = []
    
    for model in models_to_pull:
        try:
            logger.info(f"Pulling model: {model}")
            async with aiohttp.ClientSession() as session:
                async with session.post(
                    f"{OLLAMA_BASE_URL}/api/pull",
                    json={"name": model},
                    timeout=aiohttp.ClientTimeout(total=600)
                ) as response:
                    if response.status == 200:
                        results.append({"model": model, "status": "success"})
                        logger.info(f"Successfully pulled model: {model}")
                    else:
                        error_text = await response.text()
                        results.append({"model": model, "status": "error", "message": error_text})
                        logger.error(f"Failed to pull model {model}: {error_text}")
        except Exception as e:
            results.append({"model": model, "status": "error", "message": str(e)})
            logger.error(f"Exception pulling model {model}: {e}")
    
    return {"results": results}

@app.get("/api/debug/ollama")
async def debug_ollama_connection():
    """Ollama ì—°ê²° ìƒíƒœ ë””ë²„ê¹…"""
    debug_info = {
        "ollama_base_url": OLLAMA_BASE_URL,
        "embedding_model": EMBEDDING_MODEL,
        "llm_model": LLM_MODEL,
        "connection_test": None,
        "tags_response": None,
        "error": None
    }
    
    try:
        async with aiohttp.ClientSession() as session:
            # 1. ê¸°ë³¸ ì—°ê²° í…ŒìŠ¤íŠ¸
            async with session.get(f"{OLLAMA_BASE_URL}/") as response:
                debug_info["connection_test"] = {
                    "status": response.status,
                    "text": await response.text()
                }
            
            # 2. ëª¨ë¸ íƒœê·¸ í™•ì¸
            async with session.get(f"{OLLAMA_BASE_URL}/api/tags") as response:
                debug_info["tags_response"] = {
                    "status": response.status,
                    "data": await response.json() if response.status == 200 else await response.text()
                }
                
    except Exception as e:
        debug_info["error"] = str(e)
        logger.error(f"Ollama debug error: {e}")
    
    return debug_info

@app.post("/api/fallback-chat")
async def fallback_chat_endpoint(request: ChatRequest):
    """Ollama ì‹¤íŒ¨ ì‹œ ë°±ì—… ì‘ë‹µ ì‹œìŠ¤í…œ"""
    try:
        # 1. Supabaseì—ì„œ ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰
        supabase_client = get_supabase_client()
        if not supabase_client:
            return {"answer": "ì£„ì†¡í•©ë‹ˆë‹¤. í˜„ì¬ ì‹œìŠ¤í…œì— ì¼ì‹œì ì¸ ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤.", "sources": []}
        
        # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ë§¤ì¹­ìœ¼ë¡œ ê´€ë ¨ ë¬¸ì„œ ì°¾ê¸°
        keywords = request.message.lower().split()
        search_query = " | ".join(keywords[:3])  # ì²« 3ê°œ í‚¤ì›Œë“œë¡œ ê²€ìƒ‰
        
        response = supabase_client.table("documents").select("*").text_search("content", search_query).limit(3).execute()
        
        sources = []
        context_text = ""
        
        if response.data:
            for doc in response.data:
                sources.append({
                    "title": doc.get("title", "ë¬¸ì„œ"),
                    "content": doc.get("content", "")[:200] + "...",
                    "url": doc.get("url", ""),
                    "updated_at": doc.get("updated_at", "")
                })
                context_text += f"ë¬¸ì„œ: {doc.get('title', '')}\në‚´ìš©: {doc.get('content', '')[:500]}\n\n"
        
        # 2. ê·œì¹™ ê¸°ë°˜ ì‘ë‹µ ìƒì„±
        answer = generate_rule_based_answer(request.message, context_text, sources)
        
        return {
            "answer": answer,
            "sources": sources,
            "fallback_mode": True
        }
        
    except Exception as e:
        logger.error(f"Fallback chat error: {e}")
        return {
            "answer": "ì£„ì†¡í•©ë‹ˆë‹¤. í˜„ì¬ ì‹œìŠ¤í…œì— ë¬¸ì œê°€ ìˆì–´ ë‹µë³€ì„ ì œê³µí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.",
            "sources": [],
            "fallback_mode": True
        }

def generate_rule_based_answer(question: str, context: str, sources: list) -> str:
    """ê³ ë„í™”ëœ ê·œì¹™ ê¸°ë°˜ ë‹µë³€ ìƒì„±"""
    question_lower = question.lower()
    
    # ê³ ê¸‰ í‚¤ì›Œë“œ ë§¤ì¹­ ì‹œìŠ¤í…œ
    meta_keywords = ["ê´‘ê³ ", "ìº í˜ì¸", "ë©”íƒ€", "í˜ì´ìŠ¤ë¶", "ì¸ìŠ¤íƒ€ê·¸ë¨", "threads", "ë¦¬ë“œ", "ì „í™˜", "íƒ€ê²ŸíŒ…", "ì˜¤ë””ì–¸ìŠ¤"]
    policy_keywords = ["ì •ì±…", "ê°€ì´ë“œë¼ì¸", "ê·œì •", "ê·œì¹™", "ìŠ¹ì¸", "ê±°ë¶€", "ì œì¬", "ìœ„ë°˜", "ê¸ˆì§€"]
    budget_keywords = ["ì˜ˆì‚°", "ë¹„ìš©", "ê³¼ê¸ˆ", "ìš”ê¸ˆ", "ê²°ì œ", "cpc", "cpm", "cpa", "roas", "roi"]
    creative_keywords = ["í¬ë¦¬ì—ì´í‹°ë¸Œ", "ì´ë¯¸ì§€", "ë™ì˜ìƒ", "í…ìŠ¤íŠ¸", "ì œëª©", "ì„¤ëª…", "ì†Œì¬", "ë°°ë„ˆ"]
    targeting_keywords = ["íƒ€ê²ŸíŒ…", "ì˜¤ë””ì–¸ìŠ¤", "ê´€ì‹¬ì‚¬", "í–‰ë™", "ì¸êµ¬í†µê³„", "ì§€ì—­", "ì—°ë ¹", "ì„±ë³„"]
    optimization_keywords = ["ìµœì í™”", "ì„±ê³¼", "ê°œì„ ", "í–¥ìƒ", "íš¨ìœ¨", "í’ˆì§ˆ", "ì ìˆ˜", "ìˆœìœ„"]
    
    # í‚¤ì›Œë“œ ì ìˆ˜ ê³„ì‚°
    meta_score = sum(1 for keyword in meta_keywords if keyword in question_lower)
    policy_score = sum(1 for keyword in policy_keywords if keyword in question_lower)
    budget_score = sum(1 for keyword in budget_keywords if keyword in question_lower)
    creative_score = sum(1 for keyword in creative_keywords if keyword in question_lower)
    targeting_score = sum(1 for keyword in targeting_keywords if keyword in question_lower)
    optimization_score = sum(1 for keyword in optimization_keywords if keyword in question_lower)
    
    # ê°€ì¥ ë†’ì€ ì ìˆ˜ì˜ ì¹´í…Œê³ ë¦¬ ê²°ì •
    scores = {
        'meta': meta_score,
        'policy': policy_score, 
        'budget': budget_score,
        'creative': creative_score,
        'targeting': targeting_score,
        'optimization': optimization_score
    }
    
    primary_category = max(scores.keys(), key=lambda k: scores[k])
    
    # Meta ê´‘ê³  ê´€ë ¨ ì‘ë‹µ
    if primary_category == 'meta' or meta_score > 0:
        if context:
            return f"""Meta ê´‘ê³  ê´€ë ¨ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì…ë‹ˆë‹¤.

**ê´€ë ¨ ì •ë³´:**
{context[:800]}

**ì¶”ê°€ ë„ì›€:**
- ë” ìì„¸í•œ ì •ë³´ê°€ í•„ìš”í•˜ì‹œë©´ ê´€ë ¨ ë¬¸ì„œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.
- êµ¬ì²´ì ì¸ ì •ì±…ì´ë‚˜ ê°€ì´ë“œë¼ì¸ì€ ìµœì‹  Meta ë¹„ì¦ˆë‹ˆìŠ¤ ë„ì›€ë§ ì„¼í„°ë¥¼ ì°¸ì¡°í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤.

**ì¶œì²˜:** {len(sources)}ê°œì˜ ê´€ë ¨ ë¬¸ì„œì—ì„œ ì •ë³´ë¥¼ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤."""
        else:
            return """Meta ê´‘ê³ ì— ëŒ€í•œ ì¼ë°˜ì ì¸ ì•ˆë‚´ì…ë‹ˆë‹¤.

**ê¸°ë³¸ ì •ë³´:**
- Meta ê´‘ê³ ëŠ” Facebook, Instagram, Threads í”Œë«í¼ì—ì„œ ì§‘í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- ê´‘ê³  ì •ì±… ì¤€ìˆ˜ê°€ í•„ìˆ˜ì´ë©°, ì •ê¸°ì ìœ¼ë¡œ ì—…ë°ì´íŠ¸ë©ë‹ˆë‹¤.
- íƒ€ê²ŸíŒ…, ì˜ˆì‚°, í¬ë¦¬ì—ì´í‹°ë¸Œ ìµœì í™”ê°€ ì„±ê³µì˜ í•µì‹¬ì…ë‹ˆë‹¤.

ë” êµ¬ì²´ì ì¸ ì§ˆë¬¸ì„ í•´ì£¼ì‹œë©´ ë” ì •í™•í•œ ë‹µë³€ì„ ë“œë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤."""
    
    elif primary_category == 'policy':
        return f"""ğŸ“‹ **Meta ê´‘ê³  ì •ì±… ë° ê°€ì´ë“œë¼ì¸**

**ğŸ” ì£¼ìš” ì •ì±… ì‚¬í•­:**
- âœ… ëª¨ë“  ê´‘ê³ ëŠ” Metaì˜ ì»¤ë®¤ë‹ˆí‹° í‘œì¤€ ë° ê´‘ê³  ì •ì±…ì„ ì¤€ìˆ˜í•´ì•¼ í•©ë‹ˆë‹¤
- ğŸ“… ì •ì±…ì€ ì •ê¸°ì ìœ¼ë¡œ ì—…ë°ì´íŠ¸ë˜ë¯€ë¡œ ìµœì‹  ë²„ì „ í™•ì¸ì´ í•„ìˆ˜ì…ë‹ˆë‹¤
- âš ï¸ ì •ì±… ìœ„ë°˜ ì‹œ ê´‘ê³  ê±°ë¶€, ê³„ì • ì œí•œ ë˜ëŠ” ì˜êµ¬ ì •ì§€ê°€ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤
- ğŸ”„ ì •ì±… ê²€í† ëŠ” ìë™í™” ì‹œìŠ¤í…œê³¼ ìˆ˜ë™ ê²€í† ë¥¼ ë³‘í–‰í•©ë‹ˆë‹¤

**ğŸ“š ê´€ë ¨ ë¬¸ì„œ ì •ë³´:**
{context[:600] if context else 'êµ¬ì²´ì ì¸ ì •ì±… ë¬¸ì„œê°€ í•„ìš”í•˜ì‹œë©´ Meta ë¹„ì¦ˆë‹ˆìŠ¤ ë„ì›€ë§ ì„¼í„°ë¥¼ ì°¸ì¡°í•˜ì„¸ìš”.'}

**ğŸ’¡ ì¶”ê°€ ë„ì›€:**
- íŠ¹ì • ì—…ì¢…ë³„ ì •ì±…ì´ ê¶ê¸ˆí•˜ì‹œë©´ ì—…ì¢…ëª…ì„ í•¨ê»˜ ë¬¸ì˜í•´ì£¼ì„¸ìš”
- ê´‘ê³  ìŠ¹ì¸ ê±°ë¶€ ì‚¬ìœ ê°€ ê¶ê¸ˆí•˜ì‹œë©´ êµ¬ì²´ì ì¸ ìƒí™©ì„ ì•Œë ¤ì£¼ì„¸ìš”
- ì •ì±… ì—…ë°ì´íŠ¸ ì•Œë¦¼ì„ ë°›ê³  ì‹¶ìœ¼ì‹œë©´ Meta Business ë‰´ìŠ¤ë ˆí„°ë¥¼ êµ¬ë…í•˜ì„¸ìš”"""

    elif primary_category == 'budget':
        return f"""ğŸ’° **Meta ê´‘ê³  ì˜ˆì‚° ë° ë¹„ìš© ê´€ë¦¬**

**ğŸ’³ ì˜ˆì‚° ì„¤ì • ì˜µì…˜:**
- ğŸ“Š ì¼ì¼ ì˜ˆì‚°: í•˜ë£¨ì— ì†Œë¹„í•  ìµœëŒ€ ê¸ˆì•¡ ì„¤ì •
- ğŸ“ˆ ì´ ì˜ˆì‚°: ìº í˜ì¸ ì „ì²´ ê¸°ê°„ ë™ì•ˆì˜ ì´ ì†Œë¹„ í•œë„
- â° ì˜ˆì‚° ìŠ¤ì¼€ì¤„ë§: íŠ¹ì • ì‹œê°„ëŒ€/ìš”ì¼ë³„ ì˜ˆì‚° ì¡°ì • ê°€ëŠ¥

**ğŸ’¸ ê³¼ê¸ˆ ë°©ì‹:**
- ğŸ–±ï¸ CPC (í´ë¦­ë‹¹ ê³¼ê¸ˆ): ê´‘ê³  í´ë¦­ ì‹œì—ë§Œ ê³¼ê¸ˆ
- ğŸ‘€ CPM (1000íšŒ ë…¸ì¶œë‹¹ ê³¼ê¸ˆ): ê´‘ê³  ë…¸ì¶œ 1000íšŒë‹¹ ê³¼ê¸ˆ  
- ğŸ¯ CPA (ì „í™˜ë‹¹ ê³¼ê¸ˆ): ì„¤ì •í•œ ì „í™˜ ì•¡ì…˜ ë°œìƒ ì‹œì—ë§Œ ê³¼ê¸ˆ
- ğŸ“¹ ThruPlay (ë™ì˜ìƒ ì™„ì „ ì¬ìƒë‹¹ ê³¼ê¸ˆ): 15ì´ˆ ì´ìƒ ì¬ìƒ ì‹œ ê³¼ê¸ˆ

**ğŸ“Š ì„±ê³¼ ì§€í‘œ:**
- ROAS (ê´‘ê³ ë¹„ ëŒ€ë¹„ ë§¤ì¶œ): íˆ¬ì ìˆ˜ìµë¥  ì¸¡ì •
- ROI (íˆ¬ì ìˆ˜ìµë¥ ): ìˆœì´ìµ ëŒ€ë¹„ íˆ¬ì ë¹„ìš©
- CTR (í´ë¦­ë¥ ): ë…¸ì¶œ ëŒ€ë¹„ í´ë¦­ ë¹„ìœ¨
- CVR (ì „í™˜ìœ¨): í´ë¦­ ëŒ€ë¹„ ì „í™˜ ë¹„ìœ¨

**ğŸ“š ê´€ë ¨ ì •ë³´:**
{context[:400] if context else ''}

**ğŸ¯ ìµœì í™” íŒ:**
ì˜ˆì‚° íš¨ìœ¨ì„±ì„ ë†’ì´ë ¤ë©´ íƒ€ê²ŸíŒ… ì •ë°€ë„ë¥¼ ë†’ì´ê³  A/B í…ŒìŠ¤íŠ¸ë¥¼ í†µí•´ ìµœì ì˜ ì…ì°°ê°€ë¥¼ ì°¾ìœ¼ì„¸ìš”."""

    elif primary_category == 'creative':
        return f"""ğŸ¨ **Meta ê´‘ê³  í¬ë¦¬ì—ì´í‹°ë¸Œ ê°€ì´ë“œ**

**ğŸ“¸ ì´ë¯¸ì§€ ê´‘ê³ :**
- ğŸ“ ê¶Œì¥ ë¹„ìœ¨: 1:1 (ì •ì‚¬ê°í˜•), 4:5 (ì„¸ë¡œí˜•), 16:9 (ê°€ë¡œí˜•)
- ğŸ“ ìµœì†Œ í•´ìƒë„: 1080x1080px (ì •ì‚¬ê°í˜• ê¸°ì¤€)
- ğŸ“ í…ìŠ¤íŠ¸ ë¹„ìœ¨: ì´ë¯¸ì§€ ë‚´ í…ìŠ¤íŠ¸ëŠ” 20% ì´í•˜ ê¶Œì¥
- ğŸ¯ ê³ í’ˆì§ˆ ì´ë¯¸ì§€ ì‚¬ìš©ìœ¼ë¡œ ë” ë‚˜ì€ ì„±ê³¼ ë‹¬ì„±

**ğŸ¬ ë™ì˜ìƒ ê´‘ê³ :**
- â±ï¸ ê¶Œì¥ ê¸¸ì´: 15-30ì´ˆ (í”¼ë“œ), 6ì´ˆ (ìŠ¤í† ë¦¬)
- ğŸ”Š ìë§‰ í•„ìˆ˜: ë§ì€ ì‚¬ìš©ìê°€ ë¬´ìŒìœ¼ë¡œ ì‹œì²­
- ğŸ¬ ì²« 3ì´ˆê°€ í•µì‹¬: ì¦‰ì‹œ ê´€ì‹¬ì„ ëŒ ìˆ˜ ìˆëŠ” ë‚´ìš©
- ğŸ“± ëª¨ë°”ì¼ ìµœì í™”: ì„¸ë¡œ ë˜ëŠ” ì •ì‚¬ê°í˜• ë¹„ìœ¨

**âœï¸ í…ìŠ¤íŠ¸ ë° ì¹´í”¼:**
- ğŸ¯ ëª…í™•í•œ CTA (Call-to-Action): "ì§€ê¸ˆ êµ¬ë§¤", "ìì„¸íˆ ë³´ê¸°" ë“±
- ğŸ’¬ íƒ€ê²Ÿ ì˜¤ë””ì–¸ìŠ¤ ë§ì¶¤ ë©”ì‹œì§€
- ğŸ”¥ ê¸´ê¸‰ì„±ê³¼ í¬ì†Œì„± í™œìš©
- âœ¨ í˜œíƒ ì¤‘ì‹¬ì˜ ë©”ì‹œì§€ ì‘ì„±

**ğŸ“š ê´€ë ¨ ì •ë³´:**
{context[:400] if context else ''}

**ğŸ’¡ ì„±ê³¼ í–¥ìƒ íŒ:**
ì—¬ëŸ¬ í¬ë¦¬ì—ì´í‹°ë¸Œ ë²„ì „ì„ A/B í…ŒìŠ¤íŠ¸í•˜ì—¬ ìµœì ì˜ ì¡°í•©ì„ ì°¾ìœ¼ì„¸ìš”."""

    elif primary_category == 'targeting':
        return f"""ğŸ¯ **Meta ê´‘ê³  íƒ€ê²ŸíŒ… ì „ëµ**

**ğŸ‘¥ ì˜¤ë””ì–¸ìŠ¤ ìœ í˜•:**
- ğŸ¯ í•µì‹¬ ì˜¤ë””ì–¸ìŠ¤: ì¸êµ¬í†µê³„, ê´€ì‹¬ì‚¬, í–‰ë™ ê¸°ë°˜ íƒ€ê²ŸíŒ…
- ğŸ”„ ë§ì¶¤ ì˜¤ë””ì–¸ìŠ¤: ê¸°ì¡´ ê³ ê° ë°ì´í„° í™œìš© (ì´ë©”ì¼, ì „í™”ë²ˆí˜¸ ë“±)
- ğŸ‘¥ ìœ ì‚¬ ì˜¤ë””ì–¸ìŠ¤: ê¸°ì¡´ ê³ ê°ê³¼ ìœ ì‚¬í•œ íŠ¹ì„±ì˜ ì‹ ê·œ ì˜¤ë””ì–¸ìŠ¤
- ğŸ”¥ ì¬íƒ€ê²ŸíŒ…: ì›¹ì‚¬ì´íŠ¸ ë°©ë¬¸ì ë˜ëŠ” ì•± ì‚¬ìš©ì ëŒ€ìƒ

**ğŸ“Š íƒ€ê²ŸíŒ… ì˜µì…˜:**
- ğŸ“ ì§€ì—­: êµ­ê°€, ì§€ì—­, ë„ì‹œ, ë°˜ê²½ ê¸°ë°˜ íƒ€ê²ŸíŒ…
- ğŸ‘¤ ì¸êµ¬í†µê³„: ì—°ë ¹, ì„±ë³„, í•™ë ¥, ì§ì—…, ì†Œë“ ë“±
- â¤ï¸ ê´€ì‹¬ì‚¬: ì·¨ë¯¸, ì„ í˜¸ ë¸Œëœë“œ, í™œë™ ë“±
- ğŸ›’ í–‰ë™: êµ¬ë§¤ ì´ë ¥, ë””ë°”ì´ìŠ¤ ì‚¬ìš©, ì—¬í–‰ íŒ¨í„´ ë“±

**ğŸ¯ ê³ ê¸‰ íƒ€ê²ŸíŒ…:**
- â° ì‹œê°„ëŒ€ë³„ íƒ€ê²ŸíŒ…: íŠ¹ì • ì‹œê°„ì—ë§Œ ê´‘ê³  ë…¸ì¶œ
- ğŸ“± ë””ë°”ì´ìŠ¤ë³„ íƒ€ê²ŸíŒ…: ëª¨ë°”ì¼, ë°ìŠ¤í¬í†± êµ¬ë¶„
- ğŸŒ ì–¸ì–´ë³„ íƒ€ê²ŸíŒ…: ì‚¬ìš©ì ì–¸ì–´ ì„¤ì • ê¸°ë°˜
- ğŸ”— ì—°ê²° ìƒíƒœ: Wi-Fi vs ëª¨ë°”ì¼ ë°ì´í„° ì‚¬ìš©ì

**ğŸ“š ê´€ë ¨ ì •ë³´:**
{context[:400] if context else ''}

**âš¡ ìµœì í™” ì „ëµ:**
ë„ˆë¬´ ì¢ì€ íƒ€ê²ŸíŒ…ë³´ë‹¤ëŠ” ì ì • ê·œëª¨(1ë§Œëª… ì´ìƒ)ì˜ ì˜¤ë””ì–¸ìŠ¤ë¡œ ì‹œì‘í•˜ì—¬ ì„±ê³¼ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì ì§„ì ìœ¼ë¡œ ì •ë°€í™”í•˜ì„¸ìš”."""

    elif primary_category == 'optimization':
        return f"""ğŸ“ˆ **Meta ê´‘ê³  ìµœì í™” ì „ëµ**

**ğŸ” ì„±ê³¼ ë¶„ì„ ì§€í‘œ:**
- ğŸ“Š CTR (í´ë¦­ë¥ ): ë…¸ì¶œ ëŒ€ë¹„ í´ë¦­ ë¹„ìœ¨ - ì—…ê³„ í‰ê·  0.5-2%
- ğŸ’° CPC (í´ë¦­ë‹¹ ë¹„ìš©): í´ë¦­ í•œ ë²ˆë‹¹ ì§€ë¶ˆ ë¹„ìš©
- ğŸ¯ CVR (ì „í™˜ìœ¨): í´ë¦­ ëŒ€ë¹„ ì „í™˜ ë¹„ìœ¨
- ğŸ’ í’ˆì§ˆ ìˆœìœ„: ê´‘ê³  í’ˆì§ˆ, ì˜ˆìƒ ì•¡ì…˜ë¥ , ì‚¬ìš©ì ê²½í—˜ ì¢…í•© ì ìˆ˜

**âš¡ ìµœì í™” ë°©ë²•:**
- ğŸ”„ A/B í…ŒìŠ¤íŠ¸: í¬ë¦¬ì—ì´í‹°ë¸Œ, ì˜¤ë””ì–¸ìŠ¤, ë°°ì¹˜ ë“± ë‹¤ì–‘í•œ ìš”ì†Œ í…ŒìŠ¤íŠ¸
- ğŸ“± ìë™ ë°°ì¹˜: ëª¨ë“  ë°°ì¹˜ì—ì„œ ìë™ ìµœì í™” í™œìš©
- ğŸ¯ ì „í™˜ ìµœì í™”: í”½ì…€ì„ í†µí•œ ì „í™˜ ì¶”ì  ë° ìµœì í™”
- â° ì˜ˆì‚° í˜ì´ì‹±: ì˜ˆì‚°ì„ íš¨ìœ¨ì ìœ¼ë¡œ ë¶„ë°°í•˜ì—¬ ì§€ì†ì  ë…¸ì¶œ

**ğŸ“Š ìº í˜ì¸ êµ¬ì¡° ìµœì í™”:**
- ğŸ¯ ë‹¨ì¼ ëª©í‘œ: ìº í˜ì¸ë‹¹ í•˜ë‚˜ì˜ ëª…í™•í•œ ëª©í‘œ ì„¤ì •
- ğŸ“ˆ ê´‘ê³  ì„¸íŠ¸ ë¶„ë¦¬: ì„œë¡œ ë‹¤ë¥¸ ì˜¤ë””ì–¸ìŠ¤ëŠ” ë³„ë„ ê´‘ê³  ì„¸íŠ¸
- ğŸ¨ í¬ë¦¬ì—ì´í‹°ë¸Œ ë‹¤ì–‘í™”: ê´‘ê³  ì„¸íŠ¸ë‹¹ 3-5ê°œ í¬ë¦¬ì—ì´í‹°ë¸Œ
- ğŸ’° ì˜ˆì‚° ë°°ë¶„: ì„±ê³¼ ì¢‹ì€ ê´‘ê³  ì„¸íŠ¸ì— ì˜ˆì‚° ì§‘ì¤‘

**ğŸš€ ê³ ê¸‰ ìµœì í™”:**
- ğŸ¤– ìë™ ê·œì¹™: ì„±ê³¼ ê¸°ì¤€ ìë™ ì˜ˆì‚° ì¡°ì •
- ğŸ“ˆ ë™ì  ê´‘ê³ : ì œí’ˆ ì¹´íƒˆë¡œê·¸ ì—°ë™ ê°œì¸í™” ê´‘ê³ 
- ğŸ¯ ì „í™˜ API: ì„œë²„ ê°„ ì „í™˜ ë°ì´í„° ì „ì†¡ìœ¼ë¡œ ì¶”ì  ì •í™•ë„ í–¥ìƒ

**ğŸ“š ê´€ë ¨ ì •ë³´:**
{context[:400] if context else ''}

**ğŸ’¡ í•µì‹¬ íŒ:**
ìµœì†Œ 2ì£¼ê°„ ì¶©ë¶„í•œ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•œ í›„ ìµœì í™”ë¥¼ ì§„í–‰í•˜ê³ , í•œ ë²ˆì— í•˜ë‚˜ì˜ ìš”ì†Œë§Œ ë³€ê²½í•˜ì—¬ ì •í™•í•œ ì„±ê³¼ ë¶„ì„ì„ í•˜ì„¸ìš”."""
    
    
    else:
        if context:
            return f"""ì§ˆë¬¸í•´ì£¼ì‹  ë‚´ìš©ì— ëŒ€í•œ ê´€ë ¨ ì •ë³´ì…ë‹ˆë‹¤.

{context[:800]}

**ì°¸ê³ ì‚¬í•­:**
- ìœ„ ì •ë³´ëŠ” ë‚´ë¶€ ë¬¸ì„œì—ì„œ ìˆ˜ì§‘ëœ ë‚´ìš©ì…ë‹ˆë‹¤.
- ë” ì •í™•í•œ ì •ë³´ê°€ í•„ìš”í•˜ì‹œë©´ ê´€ë ¨ ë‹´ë‹¹ìì—ê²Œ ë¬¸ì˜í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤.

ì¶”ê°€ ì§ˆë¬¸ì´ ìˆìœ¼ì‹œë©´ ì–¸ì œë“  ë§ì”€í•´ì£¼ì„¸ìš”."""
        else:
            return f""""{question}"ì— ëŒ€í•œ ì§ˆë¬¸ì„ ì£¼ì…¨ìŠµë‹ˆë‹¤.

í˜„ì¬ ê´€ë ¨ëœ êµ¬ì²´ì ì¸ ë¬¸ì„œë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. 

**ë„ì›€ì´ ë  ìˆ˜ ìˆëŠ” ë°©ë²•:**
- ë” êµ¬ì²´ì ì¸ í‚¤ì›Œë“œë¡œ ì§ˆë¬¸í•´ì£¼ì„¸ìš”
- Meta ê´‘ê³ , ì •ì±…, ê°€ì´ë“œë¼ì¸ ë“± ê´€ë ¨ ìš©ì–´ë¥¼ í¬í•¨í•´ì£¼ì„¸ìš”
- íŠ¹ì • ìƒí™©ì´ë‚˜ ë¬¸ì œì— ëŒ€í•´ ìì„¸íˆ ì„¤ëª…í•´ì£¼ì„¸ìš”

ì–¸ì œë“  ë‹¤ì‹œ ì§ˆë¬¸í•´ì£¼ì‹œë©´ ë” ë‚˜ì€ ë‹µë³€ì„ ë“œë¦¬ê² ìŠµë‹ˆë‹¤."""

async def fallback_chat_response(message: str) -> ChatResponse:
    """ë°±ì—… ì±„íŒ… ì‘ë‹µ ìƒì„±"""
    try:
        # Supabaseì—ì„œ í‚¤ì›Œë“œ ê¸°ë°˜ ê²€ìƒ‰
        supabase_client = get_supabase_client()
        sources = []
        context_text = ""
        
        if supabase_client:
            keywords = message.lower().split()[:3]
            search_query = " | ".join(keywords)
            
            try:
                response = supabase_client.table("documents").select("*").text_search("content", search_query).limit(3).execute()
                
                if response.data:
                    for doc in response.data:
                        sources.append({
                            "title": doc.get("title", "ë¬¸ì„œ"),
                            "content": doc.get("content", "")[:200] + "...",
                            "url": doc.get("url", ""),
                            "updated_at": doc.get("updated_at", "")
                        })
                        context_text += f"ë¬¸ì„œ: {doc.get('title', '')}\në‚´ìš©: {doc.get('content', '')[:500]}\n\n"
            except Exception as e:
                logger.error(f"Fallback search error: {e}")
        
        # ê·œì¹™ ê¸°ë°˜ ë‹µë³€ ìƒì„±
        answer = generate_rule_based_answer(message, context_text, sources)
        
        return ChatResponse(
            answer=answer,
            sources=sources,
            response_time=1.0,
            fallback_mode=True
        )
        
    except Exception as e:
        logger.error(f"Fallback response error: {e}")
        return ChatResponse(
            answer="ì£„ì†¡í•©ë‹ˆë‹¤. í˜„ì¬ ì‹œìŠ¤í…œì— ì¼ì‹œì ì¸ ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.",
            sources=[],
            response_time=1.0,
            fallback_mode=True
        )

# Railwayì—ì„œ ì§ì ‘ ì‹¤í–‰ì„ ìœ„í•œ ì„¤ì •
def get_port():
    """Railway PORT í™˜ê²½ë³€ìˆ˜ ì•ˆì „í•œ íŒŒì‹±"""
    port_env = os.getenv('PORT', '8000')
    print(f"[STARTUP] Raw PORT environment variable: '{port_env}'")
    
    # Railwayì—ì„œ $PORTê°€ ë¬¸ìì—´ë¡œ ì „ë‹¬ë˜ëŠ” ê²½ìš° ì²˜ë¦¬
    if port_env == '$PORT' or not port_env.isdigit():
        print(f"[STARTUP] Invalid PORT value '{port_env}', using default 8000")
        return 8000
    
    try:
        port = int(port_env)
        print(f"[STARTUP] Successfully parsed PORT: {port}")
        return port
    except (ValueError, TypeError) as e:
        print(f"[STARTUP] PORT parsing error: {e}, using default 8000")
        return 8000

if __name__ == "__main__":
    import uvicorn
    import sys
    
    port = get_port()
    
    print(f"[STARTUP] Starting AdMate API server...")
    print(f"[STARTUP] Host: 0.0.0.0, Port: {port}")
    print(f"[STARTUP] OLLAMA_BASE_URL: {OLLAMA_BASE_URL}")
    print(f"[STARTUP] SUPABASE_URL: {'***' if SUPABASE_URL else 'NOT_SET'}")
    
    try:
        uvicorn.run(
            "app:app",  # ëª¨ë“ˆ:ì•± í˜•ì‹ìœ¼ë¡œ ì§€ì •
            host="0.0.0.0", 
            port=port, 
            log_level="info",
            access_log=True
        )
    except Exception as e:
        print(f"[ERROR] Server startup failed: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
# Force redeploy
# Trigger deploy after start command removal
