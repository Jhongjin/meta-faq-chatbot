import { NextRequest, NextResponse } from 'next/server';
import { llmService } from '@/lib/services/LLMService';

export async function GET(request: NextRequest) {
  try {
    console.log('ğŸ” LLM ì„œë¹„ìŠ¤ ìƒíƒœ í™•ì¸ ì¤‘...');
    
    // Ollama ì„œë¹„ìŠ¤ ìƒíƒœ í™•ì¸
    const isAvailable = await llmService.checkOllamaStatus();
    
    // ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ ì¡°íšŒ
    const models = isAvailable ? await llmService.getAvailableModels() : [];
    
    return NextResponse.json({
      success: true,
      status: {
        isAvailable,
        models,
        baseUrl: process.env.OLLAMA_BASE_URL || 'http://localhost:11434',
        defaultModel: process.env.OLLAMA_MODEL || 'qwen2.5:14b'
      }
    });

  } catch (error) {
    console.error('LLM ìƒíƒœ í™•ì¸ ì˜¤ë¥˜:', error);
    
    return NextResponse.json(
      { 
        success: false,
        error: 'LLM ì„œë¹„ìŠ¤ ìƒíƒœ í™•ì¸ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.',
        details: error instanceof Error ? error.message : String(error)
      },
      { status: 500 }
    );
  }
}
